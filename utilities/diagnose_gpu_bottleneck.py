#!/usr/bin/env python3
"""
GPU Bottleneck Diagnostic Tool - Identifies cyclic GPU utilization patterns

This tool specifically diagnoses the 2-40% oscillating GPU utilization issue
by profiling:
1. Data loading pipeline timing
2. CPU vs GPU operation placement  
3. Batch preparation vs GPU execution timing
4. Memory transfer bottlenecks
5. Real-time GPU utilization patterns

Usage:
    python utilities/diagnose_gpu_bottleneck.py
    python utilities/diagnose_gpu_bottleneck.py --batch-size 16 --num-batches 50
"""

import sys
import os
import time
import argparse
from pathlib import Path
from typing import Dict, List, Tuple
import numpy as np

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    import tensorflow as tf
except ImportError:
    print("ERROR: TensorFlow not installed")
    sys.exit(1)

from myxtts.config.config import XTTSConfig, DataConfig
from myxtts.data.ljspeech import LJSpeechDataset


class GPUBottleneckDiagnostic:
    """Diagnose GPU bottleneck causing cyclic utilization patterns."""
    
    def __init__(self, config_path: str = "configs/config.yaml"):
        """Initialize diagnostic tool."""
        self.config_path = config_path
        if Path(config_path).exists():
            self.config = XTTSConfig.from_yaml(config_path)
        else:
            print(f"‚ö†Ô∏è  Config not found: {config_path}, using defaults")
            self.config = XTTSConfig()
        
        # Timing metrics
        self.batch_times: List[float] = []
        self.data_load_times: List[float] = []
        self.gpu_wait_times: List[float] = []
        
    def check_gpu_status(self) -> bool:
        """Check if GPU is available."""
        print("\n" + "="*70)
        print("GPU STATUS CHECK")
        print("="*70)
        
        gpus = tf.config.list_physical_devices('GPU')
        if not gpus:
            print("‚ùå No GPU detected")
            return False
        
        print(f"‚úÖ Found {len(gpus)} GPU(s):")
        for i, gpu in enumerate(gpus):
            print(f"   GPU {i}: {gpu.name}")
        
        # Test GPU functionality
        try:
            with tf.device('/GPU:0'):
                a = tf.constant([[1.0]])
                b = tf.matmul(a, a)
                _ = b.numpy()
            print("‚úÖ GPU is functional")
            return True
        except Exception as e:
            print(f"‚ùå GPU test failed: {e}")
            return False
    
    def profile_data_pipeline(
        self, 
        data_path: str,
        batch_size: int = 8,
        num_batches: int = 50
    ) -> Dict[str, float]:
        """
        Profile data loading pipeline to identify bottlenecks.
        
        Args:
            data_path: Path to dataset
            batch_size: Batch size for profiling
            num_batches: Number of batches to profile
            
        Returns:
            Dictionary with profiling metrics
        """
        print("\n" + "="*70)
        print("DATA PIPELINE PROFILING")
        print("="*70)
        print(f"Batch size: {batch_size}")
        print(f"Number of batches: {num_batches}")
        
        # Create dataset
        try:
            dataset = LJSpeechDataset(
                data_path=data_path,
                config=self.config.data,
                subset="train",
                download=False
            )
            print(f"‚úÖ Dataset loaded: {len(dataset)} samples")
        except Exception as e:
            print(f"‚ùå Failed to load dataset: {e}")
            return {}
        
        # Create TensorFlow dataset
        tf_dataset = dataset.create_tf_dataset(
            batch_size=batch_size,
            shuffle=False,
            repeat=False,
            prefetch=True,
            memory_cache=False,
            num_parallel_calls=self.config.data.num_workers,
        )
        
        # Profile batch loading times
        print(f"\nProfiling {num_batches} batches...")
        iterator = iter(tf_dataset)
        
        batch_times = []
        first_batch_time = None
        
        for i in range(num_batches):
            start_time = time.perf_counter()
            try:
                batch = next(iterator)
                end_time = time.perf_counter()
                
                batch_time = (end_time - start_time) * 1000  # Convert to ms
                batch_times.append(batch_time)
                
                if i == 0:
                    first_batch_time = batch_time
                    print(f"   First batch (cold start): {batch_time:.2f}ms")
                elif i < 5:
                    print(f"   Batch {i+1}: {batch_time:.2f}ms")
                    
            except StopIteration:
                print(f"‚ö†Ô∏è  Dataset exhausted after {i} batches")
                break
            except Exception as e:
                print(f"‚ùå Error loading batch {i}: {e}")
                break
        
        if not batch_times:
            print("‚ùå No batches loaded successfully")
            return {}
        
        # Calculate statistics
        avg_time = np.mean(batch_times)
        std_time = np.std(batch_times)
        min_time = np.min(batch_times)
        max_time = np.max(batch_times)
        median_time = np.median(batch_times)
        
        # Detect oscillation pattern
        oscillation_detected = std_time > (avg_time * 0.5)  # High variance indicates oscillation
        
        print(f"\n{'='*70}")
        print("TIMING STATISTICS")
        print("="*70)
        print(f"Average batch time:  {avg_time:.2f}ms")
        print(f"Std deviation:       {std_time:.2f}ms")
        print(f"Min time:            {min_time:.2f}ms")
        print(f"Max time:            {max_time:.2f}ms")
        print(f"Median time:         {median_time:.2f}ms")
        print(f"First batch:         {first_batch_time:.2f}ms")
        
        # Analyze results
        print(f"\n{'='*70}")
        print("BOTTLENECK ANALYSIS")
        print("="*70)
        
        if oscillation_detected:
            print("üî¥ HIGH VARIATION DETECTED - Cyclic pattern identified!")
            print(f"   Std/Mean ratio: {std_time/avg_time:.2%}")
            print("   This indicates GPU is waiting for data preparation")
        else:
            print("‚úÖ Low variation - Data pipeline is stable")
        
        if avg_time > 200:
            print("üî¥ SLOW DATA LOADING DETECTED")
            print(f"   Average {avg_time:.0f}ms per batch is too slow")
            print("   GPU likely idle during data preparation")
        elif avg_time > 100:
            print("‚ö†Ô∏è  DATA LOADING IS MODERATE")
            print(f"   Average {avg_time:.0f}ms per batch could be improved")
        else:
            print("‚úÖ DATA LOADING IS FAST")
            print(f"   Average {avg_time:.0f}ms per batch is acceptable")
        
        # Check for tf.numpy_function usage
        print(f"\n{'='*70}")
        print("KNOWN ISSUES CHECK")
        print("="*70)
        
        # Check if using tf.numpy_function (CPU bottleneck)
        print("üîç Checking for tf.numpy_function usage...")
        ljspeech_file = Path(__file__).parent.parent / "myxtts" / "data" / "ljspeech.py"
        if ljspeech_file.exists():
            with open(ljspeech_file, 'r') as f:
                content = f.read()
                if 'tf.numpy_function' in content:
                    print("‚ö†Ô∏è  WARNING: tf.numpy_function found in data pipeline code")
                    print("   This CAN force CPU execution if TF-native path is not used")
                    print("   Checking if TF-native loading is enabled...")
                    
                    # Check if TF-native loading is actually being used
                    if 'use_tf_native = getattr(self.config, \'use_tf_native_loading\'' in content:
                        print("   ‚úÖ TF-native loading code path exists")
                        
                        # Check config setting
                        if hasattr(self.config.data, 'use_tf_native_loading'):
                            tf_native_enabled = self.config.data.use_tf_native_loading
                            if tf_native_enabled:
                                print(f"   ‚úÖ Config: use_tf_native_loading = {tf_native_enabled}")
                                print("   ‚úÖ TF-native loading should be active (GPU-optimized)")
                            else:
                                print(f"   üî¥ Config: use_tf_native_loading = {tf_native_enabled}")
                                print("   üî¥ CRITICAL: TF-native loading is DISABLED!")
                                print("   Action: Set use_tf_native_loading: true in config")
                        else:
                            print("   ‚ö†Ô∏è  Config setting 'use_tf_native_loading' not found")
                            print("   Will use default value from DataConfig")
                    else:
                        print("   üî¥ TF-native loading code path NOT found!")
                        print("   This means tf.numpy_function is always used (CPU bottleneck)")
                else:
                    print("‚úÖ No tf.numpy_function usage detected")
        
        # Check for TF-native loader module
        print("\nüîç Checking for TF-native loader module...")
        tf_native_file = Path(__file__).parent.parent / "myxtts" / "data" / "tf_native_loader.py"
        if tf_native_file.exists():
            print("   ‚úÖ tf_native_loader.py exists")
            # Try to import it
            try:
                import sys
                sys.path.insert(0, str(Path(__file__).parent.parent))
                from myxtts.data.tf_native_loader import TFNativeDataLoader
                print("   ‚úÖ TFNativeDataLoader can be imported")
                print("   ‚úÖ GPU-optimized data loading is available")
            except Exception as e:
                print(f"   üî¥ Failed to import TFNativeDataLoader: {e}")
                print("   üî¥ TF-native loading will fail!")
        else:
            print("   üî¥ tf_native_loader.py NOT found!")
            print("   üî¥ TF-native loading not available!")
        
        return {
            'avg_time': avg_time,
            'std_time': std_time,
            'min_time': min_time,
            'max_time': max_time,
            'median_time': median_time,
            'first_batch_time': first_batch_time,
            'oscillation_detected': oscillation_detected,
            'variation_ratio': std_time / avg_time if avg_time > 0 else 0
        }
    
    def recommend_fixes(self, metrics: Dict[str, float]):
        """Provide recommendations based on profiling results."""
        print(f"\n{'='*70}")
        print("RECOMMENDED FIXES")
        print("="*70)
        
        if metrics.get('oscillation_detected', False):
            print("\n1. üî¥ HIGH PRIORITY: Fix cyclic GPU utilization")
            print("   Problem: GPU utilization oscillates between 2-40%")
            print("   Root cause: Data pipeline bottleneck")
            print("   ")
            print("   Solutions:")
            print("   a) Replace tf.numpy_function with TensorFlow-native ops")
            print("      - Use tf.io.read_file() for file loading")
            print("      - Use tf.audio.decode_wav() for audio")
            print("      - Use tf.py_function only if absolutely necessary")
            print("   ")
            print("   b) Increase prefetch buffer size")
            print("      - Current: prefetch_buffer_size in config")
            print("      - Recommended: 8-16 batches")
            print("   ")
            print("   c) Increase num_workers for parallel loading")
            print("      - Current: num_workers in config")
            print("      - Recommended: 8-16 workers")
            print("   ")
            print("   d) Enable GPU prefetching")
            print("      - Set prefetch_to_gpu: true in config")
            print("      - Use tf.data.experimental.prefetch_to_device")
        
        avg_time = metrics.get('avg_time', 0)
        if avg_time > 100:
            print("\n2. ‚ö†Ô∏è  MEDIUM PRIORITY: Optimize data loading speed")
            print(f"   Current: {avg_time:.0f}ms per batch")
            print("   Target: <100ms per batch")
            print("   ")
            print("   Solutions:")
            print("   a) Use precomputed features (cached mel spectrograms)")
            print("   b) Increase parallel loading workers")
            print("   c) Use faster storage (SSD instead of HDD)")
            print("   d) Enable memory caching for small datasets")
        
        print("\n3. ‚úÖ CONFIGURATION CHECKLIST")
        print("   Ensure these settings in config.yaml:")
        print("   ")
        print("   data:")
        print("     use_tf_native_loading: true    # CRITICAL for GPU optimization")
        print("     num_workers: 8-16")
        print("     prefetch_buffer_size: 8-16")
        print("     prefetch_to_gpu: true")
        print("     enhanced_gpu_prefetch: true")
        print("     optimize_cpu_gpu_overlap: true")
        print("   ")
        print("   training:")
        print("     enable_graph_mode: true")
        print("     enable_xla_compilation: true")
        print("     enable_eager_debug: false")
        
        print("\n4. üîç VERIFICATION STEPS")
        print("   After configuration changes:")
        print("   1. Re-run this diagnostic tool")
        print("   2. Look for: '‚úÖ SUCCESS: Using TensorFlow-native data loading'")
        print("   3. Should NOT see: 'üî¥ WARNING: Using tf.numpy_function'")
        print("   4. Monitor GPU with: watch -n 0.5 nvidia-smi")
        print("   5. Expect: Stable 70-95% GPU utilization")
        print("   6. No more oscillation between 2-40%")


def main():
    """Main diagnostic entry point."""
    parser = argparse.ArgumentParser(
        description="Diagnose GPU bottleneck causing cyclic utilization"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="configs/config.yaml",
        help="Path to config file"
    )
    parser.add_argument(
        "--data-path",
        type=str,
        default="./data",
        help="Path to dataset"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=8,
        help="Batch size for profiling"
    )
    parser.add_argument(
        "--num-batches",
        type=int,
        default=50,
        help="Number of batches to profile"
    )
    
    args = parser.parse_args()
    
    # Run diagnostic
    diagnostic = GPUBottleneckDiagnostic(args.config)
    
    # Check GPU status
    if not diagnostic.check_gpu_status():
        print("\n‚ö†Ô∏è  GPU not available - diagnostic limited")
        return
    
    # Profile data pipeline
    metrics = diagnostic.profile_data_pipeline(
        args.data_path,
        args.batch_size,
        args.num_batches
    )
    
    # Provide recommendations
    if metrics:
        diagnostic.recommend_fixes(metrics)
    
    print("\n" + "="*70)
    print("DIAGNOSTIC COMPLETE")
    print("="*70)
    print("\nüìã QUICK FIX SUMMARY:")
    print("-" * 70)
    
    # Check current config status
    if metrics:
        if metrics.get('oscillation_detected'):
            print("üî¥ ISSUE DETECTED: GPU oscillation pattern")
            print("\n‚úÖ SOLUTION:")
            print("   1. Edit configs/config.yaml")
            print("   2. Under 'data:' section, ensure:")
            print("      use_tf_native_loading: true")
            print("   3. Save and re-run training")
            print("   4. Verify with: python utilities/diagnose_gpu_bottleneck.py")
        else:
            print("‚úÖ Data pipeline looks stable")
            print("   If still seeing GPU issues, check:")
            print("   - Batch size (may be too small)")
            print("   - Model GPU memory (may need larger allocation)")
            print("   - Dual-GPU configuration (data_gpu and model_gpu)")
    
    print("\nüìö DOCUMENTATION:")
    print("   - GPU_OSCILLATION_SOLUTION_SUMMARY.md")
    print("   - docs/GPU_OSCILLATION_FIX.md")
    print("   - DUAL_GPU_BOTTLENECK_FIX.md")
    print("\nüîß UTILITY:")
    print("   - This tool: python utilities/diagnose_gpu_bottleneck.py")
    print("   - Monitor GPU: watch -n 0.5 nvidia-smi")
    print("=" * 70)


if __name__ == "__main__":
    main()
