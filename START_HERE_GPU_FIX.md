# üö® START HERE: Fix for 1-5% GPU Utilization

## ŸÖÿ¥⁄©ŸÑ: ÿßÿ≥ÿ™ŸÅÿßÿØŸá 1-5% ÿßÿ≤ GPU ÿØÿ± ÿ≥€åÿ≥ÿ™ŸÖ Dual-RTX-4090
## Problem: 1-5% GPU Utilization on Dual RTX 4090

---

## ‚ö° QUICK FIX (5 Minutes)

```bash
# Step 1: Run the automated fix
bash quick_fix_gpu_utilization.sh

# Step 2: Train with optimized settings
python train_main.py \
    --batch-size 128 \
    --num-workers 32 \
    --enable-memory-isolation \
    --data-gpu 0 \
    --model-gpu 1 \
    --enable-static-shapes

# Step 3: Monitor GPU in another terminal
watch -n 1 nvidia-smi
```

**Expected Result:**
- GPU:0: 60-80% utilization (Data)
- GPU:1: 85-95% utilization (Model)

---

## üìã What Was Wrong?

Despite implementing:
- ‚úÖ TF-native data loading
- ‚úÖ Triple buffering  
- ‚úÖ Async pipeline
- ‚úÖ High prefetch (100+)
- ‚úÖ XLA JIT
- ‚úÖ Memory isolation

**GPU utilization was still 1-5%**

### Root Causes:
1. Batch size too small (32-64) for RTX 4090
2. Insufficient num_workers (8-16)
3. TensorFlow thread pools not configured
4. Dataset options not aggressive enough
5. Missing multi-stage prefetching

---

## üîß What the Fix Does

### 1. Aggressive TensorFlow Configuration
- Configures CPU thread pools for maximum parallelism
- Enables ALL experimental dataset optimizations
- Sets up XLA JIT and mixed precision

### 2. Optimized Data Pipeline
- Batch size: 128+ (optimized for RTX 4090)
- Workers: 32+ (maximum parallelism)
- Prefetch buffer: 100+ (aggressive buffering)
- Multi-stage prefetching (host ‚Üí GPU ‚Üí host)

### 3. Static Shapes
- Prevents tf.function retracing
- Eliminates GPU idle time during recompilation

### 4. Ultra-Aggressive Prefetching
- Host-side prefetch with AUTOTUNE
- Direct GPU prefetch with large buffer
- Final host prefetch for safety

---

## üìÅ Files Created

### Tools:
- `utilities/configure_max_gpu_utilization.py` - Apply TF optimizations
- `utilities/diagnose_gpu_utilization.py` - Diagnose issues
- `quick_fix_gpu_utilization.sh` - Automated fix script

### Modules:
- `myxtts/data/dataset_optimizer.py` - Dataset optimization functions

### Documentation:
- `CRITICAL_GPU_UTILIZATION_FIX.md` - Complete technical documentation
- `GPU_UTILIZATION_FIX_README.md` - User guide
- `START_HERE_GPU_FIX.md` - This file (quick start)

---

## üéØ Configuration Changes Required

Edit `configs/config.yaml`:

```yaml
data:
  batch_size: 128              # ‚Üê CRITICAL: Increase from 32-64
  num_workers: 32              # ‚Üê CRITICAL: Increase from 8-16
  prefetch_buffer_size: 100    # ‚Üê CRITICAL: Increase from 16-24
  
  use_tf_native_loading: true  # ‚Üê CRITICAL: Must be enabled
  prefetch_to_gpu: true        # ‚Üê CRITICAL: GPU prefetch
  pad_to_fixed_length: true    # ‚Üê CRITICAL: Static shapes
  
  enable_xla: true
  mixed_precision: true
  
  pipeline_buffer_size: 100
  shuffle_buffer_multiplier: 50
```

---

## ‚úÖ Verification

### Before Training:
```bash
# 1. Check configuration
python utilities/diagnose_gpu_utilization.py --config configs/config.yaml

# 2. Verify TensorFlow setup
python utilities/configure_max_gpu_utilization.py --verify
```

### During Training:
```bash
# Monitor GPU utilization (should be >80%)
watch -n 1 nvidia-smi

# Check training logs for:
# "‚úÖ SUCCESS: Using TensorFlow-native data loading (GPU-optimized)"
```

### If Still Low:
```bash
# Run profiler to identify bottleneck
python utilities/dual_gpu_bottleneck_profiler.py --batch-size 128 --num-steps 100
```

---

## üîç Troubleshooting

### GPU utilization still <50%?

**Try these in order:**

1. **Increase batch size:**
   ```bash
   python train_main.py --batch-size 256 ...
   ```

2. **Increase workers:**
   ```bash
   python train_main.py --num-workers 48 ...
   ```

3. **Verify TF-native loading is active:**
   - Check training logs for success message
   - If using tf.numpy_function: Major bottleneck!

4. **Check for OOM:**
   - If out of memory, reduce batch size
   - Adjust GPU memory limits

5. **Run diagnostic:**
   ```bash
   python utilities/diagnose_gpu_utilization.py --config configs/config.yaml
   ```

---

## üìä Performance Targets

| Metric | Target | Current (Issue) |
|--------|--------|-----------------|
| GPU:0 Utilization | 60-80% | 1-5% ‚ùå |
| GPU:1 Utilization | 85-95% | 1-5% ‚ùå |
| Step Time (batch=128) | <0.5s | Very high ‚ùå |
| Throughput | >200 samples/s | Very low ‚ùå |

---

## üöÄ Training Commands

### Standard Training:
```bash
python train_main.py \
    --train-data ../dataset/dataset_train \
    --val-data ../dataset/dataset_eval \
    --batch-size 128 \
    --num-workers 32 \
    --enable-static-shapes \
    --optimization-level enhanced
```

### Memory-Isolated Dual-GPU:
```bash
python train_main.py \
    --batch-size 128 \
    --num-workers 32 \
    --enable-memory-isolation \
    --data-gpu 0 \
    --model-gpu 1 \
    --data-gpu-memory 10240 \
    --model-gpu-memory 20480 \
    --enable-static-shapes \
    --optimization-level enhanced
```

### Maximum Performance:
```bash
python train_main.py \
    --batch-size 256 \
    --num-workers 48 \
    --enable-memory-isolation \
    --data-gpu 0 \
    --model-gpu 1 \
    --enable-static-shapes \
    --optimization-level enhanced \
    --prefetch-buffer-size 150
```

---

## üìö Documentation

- **Quick Start:** `START_HERE_GPU_FIX.md` (this file)
- **User Guide:** `GPU_UTILIZATION_FIX_README.md`
- **Technical Details:** `CRITICAL_GPU_UTILIZATION_FIX.md`
- **Original Issue:** `DUAL_GPU_BOTTLENECK_SOLUTION.md`

---

## üí° Key Insights

### Why was GPU utilization so low?

1. **Batch Size Mismatch:**
   - RTX 4090 can process batch_size=128 very fast
   - With batch_size=32, GPU finishes faster than data arrives
   - GPU sits idle waiting for next batch

2. **Data Pipeline Starvation:**
   - Even with TF-native loading, pipeline not aggressive enough
   - Insufficient workers and buffers
   - No multi-stage prefetching

3. **Retracing Overhead:**
   - Variable shapes cause tf.function recompilation
   - GPU idle during recompilation
   - Solution: Static shapes with padding

4. **Missing Optimizations:**
   - TensorFlow thread pools not configured
   - Experimental optimizations not enabled
   - XLA and mixed precision not active

### How does the fix work?

1. **Aggressive Batching:**
   - Larger batches keep GPU busy longer
   - Reduces pipeline overhead per sample

2. **Maximum Parallelism:**
   - More workers = more data prepared in parallel
   - Larger buffers = less chance of starvation

3. **Multi-Stage Prefetch:**
   - Host prefetch ‚Üí GPU prefetch ‚Üí Host prefetch
   - Ensures continuous data flow

4. **Static Shapes:**
   - One-time compilation
   - No retracing = no GPU idle time

5. **TensorFlow Tuning:**
   - All experimental optimizations
   - XLA kernel fusion
   - Mixed precision for speed

---

## ‚ö†Ô∏è Important Notes

1. **Memory Requirements:**
   - batch_size=128 needs ~10GB GPU memory
   - batch_size=256 needs ~18GB GPU memory
   - Adjust based on your dataset

2. **CPU Requirements:**
   - num_workers=32 needs ~16 CPU cores
   - num_workers=48 needs ~24 CPU cores
   - Scale based on your hardware

3. **First Epoch May Be Slow:**
   - TF-native cache building
   - After first run, loads from cache (very fast)

4. **Monitor Carefully:**
   - Watch nvidia-smi during first epoch
   - Check for OOM errors
   - Adjust batch size if needed

---

## ‚úÖ Success Criteria

You've successfully fixed the issue when:

- ‚úÖ GPU:0 utilization: 60-80%
- ‚úÖ GPU:1 utilization: 85-95%
- ‚úÖ Step time: Consistent and low (<0.5s for batch=128)
- ‚úÖ No "Slow operation" warnings in logs
- ‚úÖ Training logs show: "‚úÖ SUCCESS: Using TensorFlow-native data loading"
- ‚úÖ nvidia-smi shows stable, high utilization

---

## üÜò Still Having Issues?

1. **Run full diagnostic:**
   ```bash
   python utilities/diagnose_gpu_utilization.py --config configs/config.yaml
   ```

2. **Run profiler:**
   ```bash
   python utilities/dual_gpu_bottleneck_profiler.py --batch-size 128
   ```

3. **Check training logs carefully**

4. **Share diagnostic output in GitHub issue**

---

**Version:** 1.0  
**Date:** 2025-10-10  
**Target:** Dual RTX 4090 with 1-5% GPU utilization  
**Status:** Ready for Production Use
