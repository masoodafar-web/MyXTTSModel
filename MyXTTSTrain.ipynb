{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a700d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\".../dataset/dataset_train/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c5d4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 02:48:16.279394: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757719096.296785 1390661 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757719096.302163 1390661 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1757719096.317527 1390661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757719096.317545 1390661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757719096.317547 1390661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757719096.317549 1390661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-13 02:48:16.321876: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/dev371/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance monitoring started\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from myxtts.config.config import XTTSConfig,ModelConfig,DataConfig,TrainingConfig\n",
    "from myxtts import get_xtts_model, get_trainer, get_inference_engine\n",
    "from myxtts.utils.performance import start_performance_monitoring\n",
    "start_performance_monitoring()\n",
    "\n",
    "# Complete Model Configuration\n",
    "m=ModelConfig(\n",
    "    # Text encoder settings\n",
    "    text_encoder_dim=512,           # Increased for better performance\n",
    "    text_encoder_layers=6,\n",
    "    text_encoder_heads=8,\n",
    "    text_vocab_size=256_256,\n",
    "    \n",
    "    # Audio encoder settings\n",
    "    audio_encoder_dim=512,\n",
    "    audio_encoder_layers=6,\n",
    "    audio_encoder_heads=8,\n",
    "    \n",
    "    # Decoder settings\n",
    "    decoder_dim=1024,              # Increased for better quality\n",
    "    decoder_layers=12,\n",
    "    decoder_heads=16,\n",
    "    \n",
    "    # Mel spectrogram settings\n",
    "    n_mels=80,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    sample_rate=22050,\n",
    "    \n",
    "    # Voice conditioning\n",
    "    speaker_embedding_dim=256,\n",
    "    use_voice_conditioning=True,\n",
    "    \n",
    "    # Language support\n",
    "    languages=[\"en\", \"es\", \"fr\", \"de\", \"it\", \"pt\", \"pl\", \"tr\", \"ru\", \"nl\", \"cs\", \"ar\", \"zh\", \"ja\", \"hu\", \"ko\"],\n",
    "    max_text_length=500,\n",
    "    \n",
    "    # Tokenizer settings\n",
    "    tokenizer_type=\"nllb\",\n",
    "    tokenizer_model=\"facebook/nllb-200-distilled-600M\"\n",
    ")\n",
    "\n",
    "# Complete Training Configuration\n",
    "t=TrainingConfig(\n",
    "    # Training parameters\n",
    "    epochs=200,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=2000,\n",
    "    weight_decay=1e-6,\n",
    "    gradient_clip_norm=1.0,\n",
    "    \n",
    "    # Optimizer settings\n",
    "    optimizer=\"adamw\",\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    eps=1e-8,\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler=\"noam\",\n",
    "    scheduler_params={},\n",
    "    \n",
    "    # Loss weights\n",
    "    mel_loss_weight=45.0,\n",
    "    kl_loss_weight=1.0,\n",
    "    duration_loss_weight=1.0,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_step=5000,               # Save more frequently for 200 epochs\n",
    "    checkpoint_dir=\"./checkpoints\",\n",
    "    \n",
    "    # Validation\n",
    "    val_step=1000,                # Validate more frequently\n",
    "    \n",
    "    # Logging\n",
    "    log_step=100,\n",
    "    use_wandb=False,              # Disable wandb for simple training\n",
    "    wandb_project=\"myxtts\",\n",
    "    \n",
    "    # Device / distribution\n",
    "    multi_gpu=False,              # Single GPU training\n",
    "    visible_gpus=None\n",
    ")\n",
    "\n",
    "# Complete Data Configuration\n",
    "d=DataConfig(\n",
    "    # Dataset paths\n",
    "    dataset_path=\"../dataset\",     # Main dataset directory\n",
    "    dataset_name=\"custom_dataset\",\n",
    "    \n",
    "    # Custom metadata and wav paths\n",
    "    metadata_train_file=\"metadata_train.csv\",\n",
    "    metadata_eval_file=\"metadata_eval.csv\",\n",
    "    wavs_train_dir=\"wavs\",\n",
    "    wavs_eval_dir=\"wavs\",\n",
    "    \n",
    "    # Audio processing\n",
    "    sample_rate=22050,\n",
    "    trim_silence=True,\n",
    "    normalize_audio=True,\n",
    "    \n",
    "    # Text processing\n",
    "    text_cleaners=[\"english_cleaners\"],\n",
    "    language=\"en\",\n",
    "    add_blank=True,\n",
    "    \n",
    "    # Training data splits\n",
    "    train_split=0.9,\n",
    "    val_split=0.1,\n",
    "    \n",
    "    # Batch and workers\n",
    "    batch_size=4,                 # Optimized for GPU memory\n",
    "    num_workers=16,               # High for good CPU utilization\n",
    "    \n",
    "    # Voice conditioning\n",
    "    reference_audio_length=3.0,\n",
    "    min_audio_length=1.0,\n",
    "    max_audio_length=11.0,\n",
    "    \n",
    "    # Performance optimization\n",
    "    prefetch_buffer_size=8,\n",
    "    shuffle_buffer_multiplier=20,\n",
    "    enable_memory_mapping=True,\n",
    "    cache_verification=True,\n",
    "    \n",
    "    # Sequence length caps\n",
    "    max_mel_frames=512,\n",
    "    \n",
    "    # GPU optimizations\n",
    "    enable_xla=True,\n",
    "    enable_tensorrt=False,\n",
    "    mixed_precision=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    \n",
    "    # Preprocessing control\n",
    "    preprocessing_mode=\"auto\"\n",
    ")\n",
    "\n",
    "# Load configuration with complete parameters\n",
    "config = XTTSConfig(\n",
    "    model=m,\n",
    "    training=t, \n",
    "    data=d,\n",
    "    multi_gpu=False\n",
    ")\n",
    "\n",
    "# Create checkpoints directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('./checkpoints', exist_ok=True)\n",
    "\n",
    "print(\"Configuration completed with all parameters!\")\n",
    "print(f\"Model parameters: {sum(1 for _ in config.model.__dict__.items())}\")\n",
    "print(f\"Training parameters: {sum(1 for _ in config.training.__dict__.items())}\")\n",
    "print(f\"Data parameters: {sum(1 for _ in config.data.__dict__.items())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9235a902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1757719100.776982 1390661 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20786 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:4d:00.0, compute capability: 8.9\n",
      "2025-09-13 02:48:22,250 - MyXTTS - INFO - Using device: CPU\n",
      "2025-09-13 02:48:22,251 - MyXTTS - INFO - Using strategy: OneDeviceStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU setup error: Physical devices cannot be modified after being initialized\n",
      "Using single GPU strategy\n",
      "Loaded 20509 items for train subset\n",
      "Loaded 2591 items for val subset\n",
      "Precomputing mel spectrograms to ../dataset/dataset_train/processed/mels_sr22050_n80_hop256 (overwrite=False)...\n",
      "All mel spectrograms already cached.\n",
      "Precomputing mel spectrograms to ../dataset/dataset_eval/processed/mels_sr22050_n80_hop256 (overwrite=False)...\n",
      "All mel spectrograms already cached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 02:48:29,641 - MyXTTS - INFO - Cache verify: train {'checked': 20509, 'fixed': 0, 'failed': 0}, val {'checked': 2591, 'fixed': 0, 'failed': 0}\n",
      "2025-09-13 02:48:30,746 - MyXTTS - INFO - Using cached items - train: 20509, val: 2591\n",
      "2025-09-13 02:48:32,109 - MyXTTS - INFO - Training samples: 20509\n",
      "2025-09-13 02:48:32,109 - MyXTTS - INFO - Validation samples: 2591\n",
      "2025-09-13 02:48:32,110 - MyXTTS - INFO - Data loading performance:\n",
      "2025-09-13 02:48:32,110 - MyXTTS - INFO - === Data Loading Profile ===\n",
      "\n",
      "Cache Efficiency: 0.0%\n",
      "  Hits: 0\n",
      "  Misses: 0\n",
      "  Errors: 0\n",
      "\n",
      "\n",
      "2025-09-13 02:48:32,110 - MyXTTS - INFO - Starting training for 200 epochs\n",
      "2025-09-13 02:48:32,110 - MyXTTS - INFO - Current step: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance monitoring started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/5128 [00:00<?, ?it/s]/home/dev371/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'xtts', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "Epoch 0:   0%|          | 3/5128 [01:28<37:31:15, 26.36s/it, loss=169.8851, step=3, data_ms=1.4, comp_ms=20471.7, mel=3.77, stop=0.170] "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model = get_xtts_model()(config.model)\n",
    "trainer = get_trainer()(config, model)\n",
    "train_dataset, val_dataset = trainer.prepare_datasets(train_data_path=\"../dataset/dataset_train\",val_data_path=\"../dataset/dataset_eval\")\n",
    "trainer.train(train_dataset, val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340686de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enhanced Inference Section\n",
    "import os\n",
    "\n",
    "# Ensure checkpoint directory exists\n",
    "checkpoint_path = \"./checkpoints/best.ckpt\"\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(f\"Warning: Checkpoint not found at {checkpoint_path}\")\n",
    "    print(\"Make sure to train the model first or provide a valid checkpoint path\")\n",
    "    # Use the latest checkpoint if available\n",
    "    checkpoint_dir = \"./checkpoints\"\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.ckpt')]\n",
    "        if checkpoints:\n",
    "            # Sort by modification time and get the latest\n",
    "            latest_checkpoint = max(checkpoints, key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)))\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "            print(f\"Using latest checkpoint: {checkpoint_path}\")\n",
    "\n",
    "# Initialize inference engine\n",
    "try:\n",
    "    inference = get_inference_engine()(config, checkpoint_path=checkpoint_path)\n",
    "    \n",
    "    # Test synthesis with sample text\n",
    "    test_texts = [\n",
    "        \"Hello, this is a test of the MyXTTS model!\",\n",
    "        \"The weather is beautiful today.\",\n",
    "        \"Artificial intelligence is transforming the world.\"\n",
    "    ]\n",
    "    \n",
    "    # Synthesize each test text\n",
    "    for i, text in enumerate(test_texts):\n",
    "        print(f\"\\nSynthesizing text {i+1}: {text}\")\n",
    "        try:\n",
    "            result = inference.synthesize(text)\n",
    "            output_path = f\"output_sample_{i+1}.wav\"\n",
    "            inference.save_audio(result[\"audio\"], output_path)\n",
    "            print(f\"Audio saved to: {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error synthesizing text {i+1}: {e}\")\n",
    "    \n",
    "    print(\"\\nInference testing completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize inference engine: {e}\")\n",
    "    print(\"This is expected if no trained model checkpoint is available yet.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}