#!/usr/bin/env python3
"""
Direct test of mel_loss function to verify clipping fix
"""
import os
import sys
import numpy as np
import tensorflow as tf
import soundfile as sf

sys.path.insert(0, os.path.abspath('.'))

from myxtts.training.losses import mel_loss

def test_mel_loss_function():
    """Test the mel_loss function directly to see if clipping is fixed."""
    
    print("ğŸ§ª Testing Mel Loss Function Directly")
    print("=" * 50)
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´ÛŒ
    batch_size = 2
    time_steps = 100
    n_mels = 80
    
    # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ mel target ÙˆØ§Ù‚Ø¹ÛŒ (Ø¨Ø§ Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ú¯Ø³ØªØ±Ø¯Ù‡)
    mel_target = tf.random.normal([batch_size, time_steps, n_mels], mean=-5.0, stddev=2.0)
    
    # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ mel prediction Ú©Ù†ÙˆÙ†ÛŒ (Ù†Ø²Ø¯ÛŒÚ© ØµÙØ±)
    mel_pred_bad = tf.random.normal([batch_size, time_steps, n_mels], mean=0.0, stddev=0.1)
    
    # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ mel prediction Ø®ÙˆØ¨
    mel_pred_good = tf.random.normal([batch_size, time_steps, n_mels], mean=-4.8, stddev=1.8)
    
    lengths = tf.constant([time_steps, time_steps-10], dtype=tf.int32)
    
    print(f"ğŸ“Š Test Data Stats:")
    print(f"   Target: min={tf.reduce_min(mel_target):.3f}, max={tf.reduce_max(mel_target):.3f}, mean={tf.reduce_mean(mel_target):.3f}")
    print(f"   Bad Pred: min={tf.reduce_min(mel_pred_bad):.3f}, max={tf.reduce_max(mel_pred_bad):.3f}, mean={tf.reduce_mean(mel_pred_bad):.3f}")
    print(f"   Good Pred: min={tf.reduce_min(mel_pred_good):.3f}, max={tf.reduce_max(mel_pred_good):.3f}, mean={tf.reduce_mean(mel_pred_good):.3f}")
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ loss Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³Ù†Ø§Ø±ÛŒÙˆ
    print(f"\nğŸ” Testing Loss Function:")
    
    # ØªØ³Øª 1: bad prediction (Ù…Ø´Ø§Ø¨Ù‡ Ù…Ø¯Ù„ ÙØ¹Ù„ÛŒ)
    loss_bad = mel_loss(
        y_true=mel_target,
        y_pred=mel_pred_bad,
        lengths=lengths,
        label_smoothing=0.0,
        use_huber_loss=False
    )
    
    print(f"   Bad prediction loss: {loss_bad:.6f}")
    
    # ØªØ³Øª 2: good prediction
    loss_good = mel_loss(
        y_true=mel_target,
        y_pred=mel_pred_good,
        lengths=lengths,
        label_smoothing=0.0,
        use_huber_loss=False
    )
    
    print(f"   Good prediction loss: {loss_good:.6f}")
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ø³ØªÛŒ MAE Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡
    mae_bad = tf.reduce_mean(tf.abs(mel_target - mel_pred_bad))
    mae_good = tf.reduce_mean(tf.abs(mel_target - mel_pred_good))
    
    print(f"   Manual MAE (bad):  {mae_bad:.6f}")
    print(f"   Manual MAE (good): {mae_good:.6f}")
    
    # ØªØ³Øª Ú©Ù„ÛŒÙ¾ÛŒÙ†Ú¯
    print(f"\nğŸ”§ Testing Loss Clipping Behavior:")
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø®Ø·Ø§ÛŒ Ø¨Ø²Ø±Ú¯
    mel_pred_very_bad = tf.zeros_like(mel_target)  # Ù‡Ù…Ù‡ ØµÙØ±
    loss_very_bad = mel_loss(
        y_true=mel_target,
        y_pred=mel_pred_very_bad,
        lengths=lengths,
        label_smoothing=0.0,
        use_huber_loss=False
    )
    
    mae_very_bad = tf.reduce_mean(tf.abs(mel_target - mel_pred_very_bad))
    
    print(f"   Very bad prediction loss: {loss_very_bad:.6f}")
    print(f"   Manual MAE (very bad): {mae_very_bad:.6f}")
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù„ÛŒÙ¾ÛŒÙ†Ú¯
    print(f"\nğŸ“ˆ Analysis:")
    
    if loss_very_bad < 50.0:
        print(f"   âš ï¸  Loss is clipped too aggressively: {loss_very_bad:.3f}")
        print(f"   âš ï¸  Original MAE was: {mae_very_bad:.3f}")
        print(f"   âš ï¸  This may prevent learning from high-error states")
    else:
        print(f"   âœ… Loss allows high values: {loss_very_bad:.3f}")
        print(f"   âœ… This should allow proper gradient flow")
    
    # ØªØ³Øª Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†
    print(f"\nğŸ¯ Testing Gradient Flow:")
    
    # Ù…ØªØºÛŒØ± Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´
    mel_pred_var = tf.Variable(tf.zeros_like(mel_target))
    
    with tf.GradientTape() as tape:
        loss = mel_loss(
            y_true=mel_target,
            y_pred=mel_pred_var,
            lengths=lengths,
            label_smoothing=0.0,
            use_huber_loss=False
        )
    
    gradients = tape.gradient(loss, mel_pred_var)
    
    if gradients is not None:
        grad_norm = tf.norm(gradients)
        print(f"   Gradient norm: {grad_norm:.6f}")
        
        if grad_norm > 0.001:
            print(f"   âœ… Gradients are flowing properly")
        else:
            print(f"   âŒ Gradients are too small - may indicate clipping issues")
    else:
        print(f"   âŒ No gradients computed - major issue!")
    
    # Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ
    print(f"\nğŸ¯ Final Assessment:")
    
    if loss_very_bad > 10.0:
        print("   âœ… Loss clipping fix appears to be working")
        print("   âœ… High errors are no longer clipped to small values")
        print("   âœ… Model should be able to learn from mistakes")
    else:
        print("   âŒ Loss is still being clipped too aggressively")
        print("   âŒ Model may have difficulty learning mel spectrograms")
        
    return {
        'loss_bad': float(loss_bad),
        'loss_good': float(loss_good),
        'loss_very_bad': float(loss_very_bad),
        'gradient_norm': float(grad_norm) if gradients is not None else 0.0
    }

if __name__ == "__main__":
    test_mel_loss_function()