#!/usr/bin/env python3
"""
Test script to validate that the XTTS model correctly outputs duration predictions 
and attention weights for the loss functions.
"""

import os
import sys
import numpy as np
import tensorflow as tf

# Add the myxtts package to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from myxtts.models.xtts import XTTS
from myxtts.config.config import XTTSConfig
from myxtts.training.losses import XTTSLoss, duration_loss, attention_loss


def test_model_outputs():
    """Test that the model outputs the expected keys during training."""
    print("üß™ Testing XTTS model outputs for duration and attention...")
    
    # Create a simple config
    config = XTTSConfig()
    
    # Create the model
    model = XTTS(config.model)
    
    # Create dummy inputs for testing
    batch_size = 2
    text_len = 10
    mel_len = 20
    n_mels = config.model.n_mels
    
    text_inputs = tf.random.uniform([batch_size, text_len], 0, 100, dtype=tf.int32)
    mel_inputs = tf.random.normal([batch_size, mel_len, n_mels])
    text_lengths = tf.constant([text_len, text_len-2], dtype=tf.int32)
    mel_lengths = tf.constant([mel_len, mel_len-3], dtype=tf.int32)
    
    # Test training mode (should return duration_pred and attention_weights)
    print("  üìã Testing training mode...")
    outputs_train = model(
        text_inputs=text_inputs,
        mel_inputs=mel_inputs,
        text_lengths=text_lengths,
        mel_lengths=mel_lengths,
        training=True
    )
    
    required_keys = ["mel_output", "stop_tokens", "text_encoded"]
    optional_keys = ["duration_pred", "attention_weights"]
    
    # Check required outputs
    for key in required_keys:
        assert key in outputs_train, f"Missing required output: {key}"
        print(f"    ‚úÖ {key}: shape {outputs_train[key].shape}")
    
    # Check optional outputs that should be present during training
    for key in optional_keys:
        if key in outputs_train:
            print(f"    ‚úÖ {key}: shape {outputs_train[key].shape}")
        else:
            print(f"    ‚ö†Ô∏è  {key}: not present (may be expected)")
    
    # Test inference mode (should not return duration_pred and attention_weights)
    print("  üìã Testing inference mode...")
    outputs_infer = model(
        text_inputs=text_inputs,
        mel_inputs=mel_inputs,
        text_lengths=text_lengths,
        mel_lengths=mel_lengths,
        training=False
    )
    
    for key in required_keys:
        assert key in outputs_infer, f"Missing required output: {key}"
        print(f"    ‚úÖ {key}: shape {outputs_infer[key].shape}")
    
    # Duration predictions and attention weights should not be present in inference
    for key in optional_keys:
        if key in outputs_infer:
            print(f"    ‚ö†Ô∏è  {key}: present in inference mode (unexpected)")
        else:
            print(f"    ‚úÖ {key}: correctly absent in inference mode")
    
    return outputs_train, outputs_infer


def test_loss_computation():
    """Test that the loss functions work with the model outputs."""
    print("\nüß™ Testing loss computation with model outputs...")
    
    # Create a loss function with enabled weights
    loss_fn = XTTSLoss(
        mel_loss_weight=35.0,
        stop_loss_weight=1.0,
        attention_loss_weight=1.0,
        duration_loss_weight=0.1
    )
    
    # Create dummy model outputs and targets
    batch_size = 2
    text_len = 10
    mel_len = 20
    n_mels = 80
    
    # Model outputs (simulating training mode)
    y_pred = {
        "mel_output": tf.random.normal([batch_size, mel_len, n_mels]),
        "stop_tokens": tf.random.uniform([batch_size, mel_len, 1], 0, 1),
        "duration_pred": tf.random.uniform([batch_size, text_len], 0.1, 2.0),
        "attention_weights": tf.random.uniform([batch_size, 8, mel_len, text_len], 0, 1)  # 8 heads
    }
    
    # Make attention weights sum to 1 over text dimension (proper attention weights)
    y_pred["attention_weights"] = tf.nn.softmax(y_pred["attention_weights"], axis=-1)
    
    # Target values
    y_true = {
        "mel_target": tf.random.normal([batch_size, mel_len, n_mels]),
        "stop_target": tf.concat([
            tf.zeros([batch_size, mel_len-1, 1]),
            tf.ones([batch_size, 1, 1])
        ], axis=1),
        "duration_target": tf.random.uniform([batch_size, text_len], 0.5, 1.5),
        "text_lengths": tf.constant([text_len, text_len-2], dtype=tf.int32),
        "mel_lengths": tf.constant([mel_len, mel_len-3], dtype=tf.int32)
    }
    
    # Compute loss
    total_loss = loss_fn(y_true, y_pred)
    
    print(f"    ‚úÖ Total loss computed: {total_loss.numpy():.4f}")
    
    # Check individual losses
    individual_losses = loss_fn.losses
    for loss_name, loss_value in individual_losses.items():
        print(f"    üìä {loss_name}: {loss_value.numpy():.4f}")
    
    # Test individual loss functions
    print("\n  üìã Testing individual loss functions...")
    
    # Test duration loss
    dur_loss = duration_loss(
        y_pred["duration_pred"],
        y_true["duration_target"],
        y_true["text_lengths"]
    )
    print(f"    ‚úÖ Duration loss: {dur_loss.numpy():.4f}")
    
    # Test attention loss
    attn_loss = attention_loss(
        y_pred["attention_weights"],
        y_true["text_lengths"],
        y_true["mel_lengths"]
    )
    print(f"    ‚úÖ Attention loss: {attn_loss.numpy():.4f}")
    
    return total_loss


def test_model_without_optional_outputs():
    """Test that the loss function handles models without duration/attention outputs gracefully."""
    print("\nüß™ Testing loss computation without optional outputs...")
    
    loss_fn = XTTSLoss(
        mel_loss_weight=35.0,
        stop_loss_weight=1.0,
        attention_loss_weight=1.0,
        duration_loss_weight=0.1
    )
    
    batch_size = 2
    mel_len = 20
    n_mels = 80
    
    # Model outputs without duration_pred and attention_weights (simulating old model behavior)
    y_pred = {
        "mel_output": tf.random.normal([batch_size, mel_len, n_mels]),
        "stop_tokens": tf.random.uniform([batch_size, mel_len, 1], 0, 1)
    }
    
    # Target values
    y_true = {
        "mel_target": tf.random.normal([batch_size, mel_len, n_mels]),
        "stop_target": tf.concat([
            tf.zeros([batch_size, mel_len-1, 1]),
            tf.ones([batch_size, 1, 1])
        ], axis=1),
        "mel_lengths": tf.constant([mel_len, mel_len-3], dtype=tf.int32)
    }
    
    # Compute loss (should work without duration/attention components)
    total_loss = loss_fn(y_true, y_pred)
    
    print(f"    ‚úÖ Total loss without optional outputs: {total_loss.numpy():.4f}")
    
    individual_losses = loss_fn.losses
    for loss_name, loss_value in individual_losses.items():
        print(f"    üìä {loss_name}: {loss_value.numpy():.4f}")
    
    # Verify that duration and attention losses are not computed
    assert "duration_loss" not in individual_losses, "Duration loss should not be computed"
    assert "attention_loss" not in individual_losses, "Attention loss should not be computed"
    print("    ‚úÖ Duration and attention losses correctly skipped")
    
    return total_loss


def main():
    """Run all tests."""
    print("üöÄ Starting XTTS Duration and Attention Output Tests")
    print("=" * 60)
    
    try:
        # Test model outputs
        outputs_train, outputs_infer = test_model_outputs()
        
        # Test loss computation with new outputs
        loss_with_outputs = test_loss_computation()
        
        # Test loss computation without optional outputs
        loss_without_outputs = test_model_without_optional_outputs()
        
        print("\n" + "=" * 60)
        print("üéâ All tests passed!")
        print(f"   Model correctly outputs duration predictions and attention weights during training")
        print(f"   Loss functions handle both old and new model outputs gracefully")
        print(f"   Training stability should be improved with monotonic attention guidance")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)