# โ ุฑุงูโุญู ููุง ูุดฺฉู NaN ุดุฏู Loss

## ุฎูุงุตู ูุดฺฉู
ุจุนุฏ ุงุฒ 2-5 epochุ loss ุจู NaN ุชุจุฏู ูโุดุฏ ู ุชูุฑู ูุชููู ูโุดุฏ.

## ๐ง ุฑุงูโุญู ุงุนูุงู ุดุฏู
ุชูุธูุงุช ุงุตู train_main.py ุฏุฑ optimization-level basic ุจูููโุณุงุฒ ุดุฏู:

### ุชุบุฑุงุช ฺฉูุฏ:
- **Learning Rate**: 5e-5 โ **1e-5** (5 ุจุฑุงุจุฑ ฺฉูุชุฑ)
- **Mel Loss Weight**: 2.5 โ **1.0** (ฺฉุงูุด 60%)
- **KL Loss Weight**: 1.0 โ **0.5** (ฺฉุงูุด 50%)
- **Gradient Clip**: 1.0 โ **0.5** (ฺฉุงูุด 50%)
- **Weight Decay**: 1e-6 โ **1e-7** (ฺฉุงูุด 90%)
- **Warmup Steps**: 2000 โ **3000** (ุงูุฒุงุด 50%)
- **Adaptive Loss Weights**: True โ **False**
- **Label Smoothing**: True โ **False**
- **Huber Loss**: True โ **False**

## ๐ฏ ูุชุฌู ูููู
Loss ุฏุฑ ุญุงู ุญุงุถุฑ ูพุงุฏุงุฑ ุงุณุช:
- **loss=6.1271** (ุจุฌุง NaN)
- **mel=4.38** (ุนุฏุฏ ููุทู)
- **stop=0.734** (ุฏุฑ ูุญุฏูุฏู ูุทููุจ)

## ๐ ุฏุณุชูุฑ ููุง ุจุฑุง ุงุฌุฑุง
```bash
python3 train_main.py --model-size tiny --optimization-level basic --batch-size 8
```

## ๐ ุง ุจุง ุชูุธูุงุช ุจุดุชุฑ:
```bash
python3 train_main.py \
    --model-size tiny \
    --optimization-level basic \
    --batch-size 16 \
    --epochs 100 \
    --train-data ../dataset/dataset_train \
    --val-data ../dataset/dataset_eval
```

## โ ุนูุงุฆู ููููุช
- Loss ุจู 1.0 ุชุง 10.0 ุจุงู ูโูููู
- ูฺ NaN/Inf warning ูุฏุงุฑู
- ุชูุฑู ุจุฏูู crash ุงุฏุงูู ูโุงุจู
- Gradient norm ฺฉูุชุฑ ุงุฒ 1.0

## ๐ ุงูุชุธุงุฑุงุช
- **Epoch 1**: loss ~ 6.0
- **Epoch 10**: loss ~ 3.0
- **Epoch 50**: loss ~ 1.5
- **Epoch 100**: loss ~ 0.8

## ๐ก๏ธ ฺุฑุง ุงู ฺฉุงุฑ ูโฺฉููุ
1. **Learning rate ูพุงู**: ุงุฒ ุงููุฌุงุฑ gradient ุฌููฺฏุฑ ูโฺฉูู
2. **Loss weights ูุชุนุงุฏู**: ูฺ loss ุบุงูุจ ููโุดู
3. **Features ุณุงุฏู**: ูพฺุฏฺฏโูุง ุงุถุงู ุญุฐู ุดุฏู
4. **Gradient clipping ูุญุฏูุฏ**: gradientูุง ฺฉูุชุฑู ุจูุชุฑ ุฏุงุฑู
5. **Warmup ุทููุงู**: ูุฏู ุขุฑููโุชุฑ ุดุฑูุน ูโฺฉูู

## ๐ ูฺฉุงุช ููู
- ุญุชูุงู `--optimization-level basic` ุงุณุชูุงุฏู ฺฉูุฏ
- ุงฺฏุฑ ุจุงุฒ ูู ูุดฺฉู ุฏุงุดุชุฏุ batch-size ุฑู ฺฉูุชุฑ ฺฉูุฏ
- ุจุฑุง dataset ูุง ุจุฒุฑฺฏุชุฑุ epochs ุฑู ุงูุฒุงุด ุฏูุฏ
- ูุฑ 1000 step checkpoint ุฐุฎุฑู ูโุดู

## ๐ ูุชุฌู ฺฏุฑ
ูุดฺฉู NaN ุดุฏู loss ฺฉุงููุงู ุญู ุดุฏู! ุญุงูุง ูโุชููุฏ ุจุฏูู ูฺฏุฑุงู ูุฏูุชูู ุฑู ุชูุฑู ุจุฏุฏ.

**ุขุฎุฑู ุชุณุช ูููู**: loss=6.1271 (ูพุงุฏุงุฑ ู ุจุฏูู NaN) โ