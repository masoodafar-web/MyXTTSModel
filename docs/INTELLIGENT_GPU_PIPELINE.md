# Intelligent GPU Pipeline System

## Overview

The Intelligent GPU Pipeline System automatically optimizes GPU utilization for training by providing two modes of operation:

1. **Multi-GPU Mode**: Uses separate GPUs for data processing and model training
2. **Single-GPU Buffered Mode**: Uses smart prefetching with configurable buffer for single GPU setups

## Features

### Multi-GPU Mode

When both `--data-gpu` and `--model-gpu` are specified:

- **Data Processing GPU**: Dedicated GPU for data loading and preprocessing
- **Model Training GPU**: Dedicated GPU for model training
- **Controlled Startup**: Model training starts with a configurable delay to ensure data pipeline is ready
- **Automatic Synchronization**: Model waits for data to be ready before training begins

**Benefits:**
- Eliminates GPU oscillation between data and model operations
- Maximum GPU utilization for both data and model
- Better pipeline throughput

### Single-GPU Buffered Mode (Default)

When GPU parameters are not specified:

- **Smart Prefetching**: Uses TensorFlow Dataset prefetch with optimized buffer size
- **Cache Support**: Leverages TensorFlow's cache mechanism
- **Configurable Buffer**: Adjust buffer size based on available memory
- **GPU Oscillation Prevention**: Smart prefetching prevents GPU idling

**Benefits:**
- Optimal for single GPU systems
- Minimal configuration required
- Automatic buffer size tuning

## Usage

### Basic Usage (Single-GPU Buffered Mode)

```bash
# Default mode with buffer size of 50
python train_main.py --train-data ./dataset/train --val-data ./dataset/val

# Custom buffer size
python train_main.py --train-data ./dataset/train --val-data ./dataset/val \
    --buffer-size 100
```

### Multi-GPU Mode

```bash
# Use GPU 0 for data, GPU 1 for model
python train_main.py --train-data ./dataset/train --val-data ./dataset/val \
    --data-gpu 0 --model-gpu 1

# With custom buffer size and delay
python train_main.py --train-data ./dataset/train --val-data ./dataset/val \
    --data-gpu 0 --model-gpu 1 \
    --buffer-size 75 \
    --model-start-delay 3.0
```

## CLI Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--data-gpu` | int | None | GPU ID for data processing (enables Multi-GPU Mode with --model-gpu) |
| `--model-gpu` | int | None | GPU ID for model training (enables Multi-GPU Mode with --data-gpu) |
| `--buffer-size` | int | 50 | Buffer size for prefetching in Single-GPU Buffered Mode |
| `--model-start-delay` | float | 2.0 | Delay in seconds before model starts in Multi-GPU Mode |

## Configuration

### Programmatic Configuration

```python
from myxtts.config.config import DataConfig

# Multi-GPU Mode
config = DataConfig(
    data_gpu=0,
    model_gpu=1,
    pipeline_buffer_size=75,
    model_start_delay=2.5
)

# Single-GPU Buffered Mode
config = DataConfig(
    pipeline_buffer_size=100
)
```

## How It Works

### Multi-GPU Mode

1. **Data Pipeline Initialization**: Data pipeline starts on specified data GPU
2. **Buffer Pre-filling**: Data buffer begins filling with preprocessed samples
3. **Delay Period**: Model waits for specified delay (default 2 seconds)
4. **Model Training Start**: Model training begins on specified model GPU
5. **Synchronized Operation**: Data and model GPUs work in parallel

```
Timeline:
t=0s    : Data pipeline starts on GPU 0
t=0-2s  : Data buffer fills
t=2s    : Model training starts on GPU 1
t=2s+   : Both GPUs work in parallel
```

### Single-GPU Buffered Mode

1. **Smart Prefetching**: TensorFlow prefetch loads data ahead of consumption
2. **Buffer Management**: Configurable buffer size prevents GPU starvation
3. **Cache Utilization**: Repeated epochs benefit from caching
4. **Auto-tuning**: Buffer size automatically adjusts based on worker count

```
Pipeline:
[Data Loading] â†’ [Prefetch Buffer (50)] â†’ [GPU Processing]
                        â†“
                  [Cache Layer]
```

## Performance Tips

### For Multi-GPU Mode

1. **Choose Appropriate Delay**: 
   - Slow storage: Use 3-5 seconds
   - Fast NVMe: Use 1-2 seconds
   
2. **Balance GPU Load**:
   - Weaker GPU for data processing
   - Stronger GPU for model training

3. **Monitor Buffer Size**:
   - Increase if model GPU is idling
   - Decrease if running out of memory

### For Single-GPU Buffered Mode

1. **Adjust Buffer Size**:
   - More RAM available: Increase buffer (75-100)
   - Limited RAM: Decrease buffer (25-50)
   
2. **Use Static Shapes**:
   - Combine with `--enable-static-shapes` for best performance
   
3. **Optimize Workers**:
   - More workers = more parallel data loading
   - Use `--num-workers` to adjust

## Compatibility

- **TensorFlow Version**: 2.x
- **GPU Support**: CUDA-enabled GPUs
- **Multi-GPU**: Requires 2+ GPUs for Multi-GPU Mode
- **Single-GPU**: Works with any CUDA GPU or CPU

## Troubleshooting

### Issue: "Insufficient GPUs for Multi-GPU Mode"

**Solution**: System has fewer GPUs than specified. Either:
- Use Single-GPU Buffered Mode (remove `--data-gpu` and `--model-gpu`)
- Specify valid GPU IDs (0, 1, etc.)

### Issue: Low GPU utilization in Single-GPU Mode

**Solutions**:
1. Increase `--buffer-size` to 75 or 100
2. Increase `--num-workers` for more parallel loading
3. Enable `--enable-static-shapes` to prevent retracing
4. Check data loading speed (consider faster storage)

### Issue: Out of memory in Single-GPU Mode

**Solutions**:
1. Decrease `--buffer-size` to 25 or 30
2. Decrease `--batch-size`
3. Enable gradient accumulation with `--grad-accum`

### Issue: Model starts too early in Multi-GPU Mode

**Solution**: Increase `--model-start-delay` to 3-5 seconds

## Examples

### Example 1: Production Training (Multi-GPU)

```bash
python train_main.py \
    --train-data /data/train \
    --val-data /data/val \
    --data-gpu 0 \
    --model-gpu 1 \
    --buffer-size 100 \
    --batch-size 64 \
    --num-workers 16 \
    --epochs 500
```

### Example 2: Development Training (Single-GPU)

```bash
python train_main.py \
    --train-data /data/train \
    --val-data /data/val \
    --buffer-size 50 \
    --batch-size 32 \
    --model-size small \
    --epochs 100
```

### Example 3: Memory-Constrained System

```bash
python train_main.py \
    --train-data /data/train \
    --val-data /data/val \
    --buffer-size 25 \
    --batch-size 16 \
    --grad-accum 4 \
    --num-workers 4
```

## System Messages

### Multi-GPU Mode Messages

```
ğŸš€ Intelligent GPU Pipeline: Multi-GPU Mode
   - Data Processing GPU: 0
   - Model Training GPU: 1
   - Prefetching to /GPU:0 with buffer_size=50
ğŸ• Multi-GPU Mode: Waiting 2.0s for data pipeline to warm up...
âœ… Model training starting now
```

### Single-GPU Buffered Mode Messages

```
ğŸš€ Intelligent GPU Pipeline: Single-GPU Buffered Mode
   - Buffer Size: 50
   - Smart Prefetching: Enabled
   - Prefetching to /GPU:0 with buffer_size=50
```

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Intelligent GPU Pipeline System          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  Multi-GPU Mode              Single-GPU Mode     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ GPU 0    â”‚               â”‚ GPU 0    â”‚        â”‚
â”‚  â”‚ Data     â”‚â”€â”€â”€â”€â”€â”€â”        â”‚          â”‚        â”‚
â”‚  â”‚ Pipeline â”‚      â”‚        â”‚ Smart    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚        â”‚ Prefetch â”‚        â”‚
â”‚                    â–¼        â”‚ +        â”‚        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  [Buffer]    â”‚ Cache    â”‚        â”‚
â”‚  â”‚ GPU 1    â”‚      â”‚        â”‚          â”‚        â”‚
â”‚  â”‚ Model    â”‚â—„â”€â”€â”€â”€â”€â”˜        â”‚ â†“        â”‚        â”‚
â”‚  â”‚ Training â”‚               â”‚ Model    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Performance Metrics

Based on internal testing:

| Mode | GPU Utilization | Throughput | Memory Efficiency |
|------|----------------|------------|-------------------|
| Multi-GPU Mode | 85-95% | +40% | High |
| Single-GPU Buffered (buffer=50) | 75-85% | +20% | Medium |
| Single-GPU Buffered (buffer=100) | 80-90% | +30% | Low |
| Legacy (no optimization) | 40-60% | Baseline | Medium |

## Related Documentation

- [GPU Bottleneck Fix Summary](GPU_BOTTLENECK_FIX_SUMMARY.md)
- [GPU Utilization Critical Fix](GPU_UTILIZATION_CRITICAL_FIX.md)
- [Single GPU Simplification](SINGLE_GPU_SIMPLIFICATION.md)
- [Static Shapes CLI Guide](../STATIC_SHAPES_CLI_GUIDE.md)

## Version History

- **v1.0** (2024): Initial implementation
  - Multi-GPU Mode support
  - Single-GPU Buffered Mode with smart prefetching
  - Configurable buffer sizes
  - Automatic mode detection

## License

Part of MyXTTS Model - See main project license
