data:
  add_blank: true
  batch_size: 32
  dataset_name: ljspeech
  dataset_path: ./data/ljspeech
  language: en
  max_audio_length: 11.0
  min_audio_length: 1.0
  normalize_audio: true
  num_workers: 4
  reference_audio_length: 3.0
  sample_rate: 22050
  text_cleaners:
  - english_cleaners
  train_split: 0.9
  trim_silence: true
  val_split: 0.1
  # Custom metadata and wav directory options (optional)
  # If both metadata_train_file and metadata_eval_file are set, 
  # they will be used with potentially different wav directories
  # metadata_train_file: ./data/custom_train/metadata_train.csv
  # metadata_eval_file: ./data/custom_eval/metadata_eval.csv
  # wavs_train_dir: ./data/custom_train/wavs
  # wavs_eval_dir: ./data/custom_eval/wavs
  
  # Dataset preprocessing control
  preprocessing_mode: auto     # "auto", "precompute", "runtime"
model:
  audio_encoder_dim: 512
  audio_encoder_heads: 8
  audio_encoder_layers: 6
  decoder_dim: 1024
  decoder_heads: 16
  decoder_layers: 12
  hop_length: 256
  languages:
  - en
  - es
  - fr
  - de
  - it
  - pt
  - pl
  - tr
  - ru
  - nl
  - cs
  - ar
  - zh
  - ja
  - hu
  - ko
  max_text_length: 500
  n_fft: 1024
  n_mels: 80
  sample_rate: 22050
  speaker_embedding_dim: 256
  text_encoder_dim: 512
  text_encoder_heads: 8
  text_encoder_layers: 6
  text_vocab_size: 256256
  tokenizer_model: facebook/nllb-200-distilled-600M
  tokenizer_type: nllb
  use_voice_conditioning: true
  win_length: 1024
training:
  beta1: 0.9
  beta2: 0.999
  checkpoint_dir: ./checkpoints
  duration_loss_weight: 1.0
  epochs: 200
  eps: 1.0e-08
  gradient_clip_norm: 1.0
  kl_loss_weight: 1.0
  learning_rate: 5.0e-05
  log_step: 100
  mel_loss_weight: 45.0
  optimizer: adamw
  save_step: 1000
  scheduler: noam
  scheduler_params: {}
  use_wandb: false
  val_step: 500
  wandb_project: myxtts
  warmup_steps: 4000
  weight_decay: 1.0e-06
