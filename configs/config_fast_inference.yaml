# Fast Inference Configuration for MyXTTS with Neural Vocoder
# Optimized for speed using non-autoregressive decoding and efficient vocoder

model:
  # Text encoder settings - Balanced
  text_encoder_dim: 512
  text_encoder_layers: 6
  text_encoder_heads: 8
  text_vocab_size: 256256
  
  # Audio encoder settings - Efficient
  audio_encoder_dim: 512
  audio_encoder_layers: 6
  audio_encoder_heads: 8
  
  # Decoder settings - Fast non-autoregressive
  decoder_strategy: "non_autoregressive"
  decoder_dim: 1024
  decoder_layers: 12
  decoder_heads: 16
  
  # Neural vocoder settings - Efficient
  vocoder_type: "hifigan"
  vocoder_upsample_rates: [8, 8, 2, 2]
  vocoder_upsample_kernel_sizes: [16, 16, 4, 4]
  vocoder_resblock_kernel_sizes: [3, 7, 11]
  vocoder_resblock_dilation_sizes: [[1, 3, 5], [1, 3, 5], [1, 3, 5]]
  vocoder_initial_channel: 256  # Reduced for speed
  
  # Mel spectrogram settings
  n_mels: 80
  n_fft: 1024
  hop_length: 256
  win_length: 1024
  sample_rate: 22050
  
  # Voice conditioning - Streamlined
  speaker_embedding_dim: 256
  use_voice_conditioning: true
  voice_conditioning_layers: 2
  voice_similarity_threshold: 0.7
  enable_voice_adaptation: true
  voice_encoder_dropout: 0.1
  
  # Advanced features - Optimized for speed
  enable_gradient_checkpointing: true
  max_attention_sequence_length: 512
  use_memory_efficient_attention: true

training:
  # Two-stage training configuration
  use_two_stage_training: true
  
  # Stage 1: Text-to-Mel (Non-autoregressive)
  stage1_epochs: 80
  stage1_learning_rate: 2e-4
  stage1_checkpoint_path: "checkpoints/fast_stage1"
  
  # Stage 2: Mel-to-Audio (Efficient Vocoder)
  stage2_epochs: 150
  stage2_learning_rate: 3e-4
  stage2_checkpoint_path: "checkpoints/fast_stage2"
  
  # General training settings - Optimized for speed
  batch_size: 64  # Larger batches for efficiency
  num_workers: 12
  enable_mixed_precision: true
  gradient_clip_norm: 1.0
  weight_decay: 0.01
  
  # Loss weights
  mel_loss_weight: 1.0
  duration_loss_weight: 1.0  # For non-autoregressive training
  vocoder_mel_loss_weight: 30.0  # Reduced for faster convergence
  vocoder_feature_loss_weight: 1.5

data:
  dataset_path: ""
  dataset_name: "ljspeech"
  sample_rate: 22050
  trim_silence: true
  normalize_audio: true
  language: "en"
  
  # Batching optimized for speed
  batch_size: 64
  num_workers: 12
  prefetch_buffer_size: 16
  shuffle_buffer_multiplier: 30

# Usage notes:
# This configuration prioritizes inference speed over maximum quality
# Suitable for real-time applications and development
# 3-5x faster inference than autoregressive approach
# Still provides high-quality audio with neural vocoder
# Good balance of speed and quality for most applications