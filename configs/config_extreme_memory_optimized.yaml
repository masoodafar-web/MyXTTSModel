# Extreme Memory-Optimized Configuration for MyXTTS
# This configuration prevents OOM errors on GPUs with limited memory (8-12GB)

model:
  # Very reduced model dimensions to save memory
  text_encoder_dim: 128         # Heavily reduced from 512
  text_encoder_layers: 4        # Reduced from 6
  text_encoder_heads: 4         # Reduced from 8
  
  audio_encoder_dim: 128        # Heavily reduced from 512
  audio_encoder_layers: 4       # Reduced from 6
  audio_encoder_heads: 4        # Reduced from 8
  
  decoder_dim: 256              # Heavily reduced from 1024
  decoder_layers: 6             # Reduced from 12
  decoder_heads: 8              # Reduced from 16
  
  n_mels: 80
  sample_rate: 22050
  use_voice_conditioning: true
  max_text_length: 256          # Heavily reduced from 500
  
  # Memory optimization settings
  enable_gradient_checkpointing: true    # Enable gradient checkpointing
  max_attention_sequence_length: 256     # Heavily limit attention sequence length

data:
  # Extreme memory-optimized data settings
  batch_size: 1                # Minimal batch size
  num_workers: 1               # Minimal workers
  prefetch_buffer_size: 1      # Minimal buffering
  shuffle_buffer_multiplier: 5 # Minimal shuffling
  
  # Audio processing
  sample_rate: 22050
  n_mels: 80
  normalize_audio: true
  max_mel_frames: 256          # Reduced from 384
  
  # Memory optimization flags
  enable_memory_mapping: false  # Disable to save GPU memory
  prefetch_to_gpu: false       # Keep data on CPU
  mixed_precision: true        # Enable for memory efficiency
  enable_xla: false           # Disable to save compilation memory
  
  # Dataset files
  metadata_train_file: "metadata_train.csv"
  metadata_eval_file: "metadata_eval.csv"
  wavs_train_dir: "wavs"
  wavs_eval_dir: "wavs"

training:
  # Basic training settings
  epochs: 200
  learning_rate: 3e-5          # Slightly reduced
  warmup_steps: 1000           # Reduced warmup
  
  # Memory optimization
  gradient_accumulation_steps: 32  # Very high accumulation to simulate batch size 32
  enable_memory_cleanup: true      # Clean memory between batches
  max_memory_fraction: 0.65        # Use only 65% of GPU memory
  
  # Optimizer
  optimizer: "adamw"
  weight_decay: 1e-6
  gradient_clip_norm: 0.25     # Very conservative gradient clipping
  
  # GPU settings
  multi_gpu: false              # Disable for single GPU training
  visible_gpus: null           # Use default GPU
  
  # Checkpointing and logging
  save_step: 10000             # More frequent saves
  val_step: 2500               # More frequent validation
  log_step: 50                 # More frequent logging
  checkpoint_dir: "./checkpoints"
  
  # Weights & Biases
  use_wandb: false
  wandb_project: "myxtts"
  
  # Fixed loss weights (addresses "لاس سه رقمیه" issue)
  mel_loss_weight: 2.5   # Fixed from 45.0 - was causing three-digit loss values
  kl_loss_weight: 1.0
  duration_loss_weight: 1.0
