
# MyXTTS Stable Training Configuration - NaN Loss Fix
# تنظیمات پایدار برای جلوگیری از NaN شدن loss

model:
  # Tiny model settings optimized for stability
  text_encoder_dim: 256
  text_encoder_layers: 4
  text_encoder_heads: 4
  
  audio_encoder_dim: 256
  audio_encoder_layers: 4
  audio_encoder_heads: 4
  
  decoder_dim: 768
  decoder_layers: 8
  decoder_heads: 12
  
  # Memory optimizations
  enable_gradient_checkpointing: true
  max_attention_sequence_length: 256
  use_memory_efficient_attention: true

training:
  # STABLE LEARNING RATE (key fix)
  learning_rate: 1e-05
  
  # BALANCED LOSS WEIGHTS (prevents explosion)
  mel_loss_weight: 1.0
  kl_loss_weight: 0.5
  duration_loss_weight: 0.1
  pitch_loss_weight: 0.05
  energy_loss_weight: 0.05
  
  # Voice cloning weights (reduced)
  voice_similarity_loss_weight: 0.5
  speaker_classification_loss_weight: 0.3
  voice_reconstruction_loss_weight: 0.4
  
  # CONSERVATIVE GRADIENTS (prevents explosion)
  gradient_clip_norm: 0.3
  weight_decay: 1e-07
  
  # OPTIMIZER SETTINGS
  optimizer: "adamw"
  beta1: 0.9
  beta2: 0.999
  eps: 1e-08
  
  # SCHEDULER SETTINGS
  scheduler: "noam"
  warmup_steps: 3000
  
  # STABILITY FEATURES
  use_adaptive_loss_weights: false
  loss_smoothing_factor: 0.05
  max_loss_spike_threshold: 1.2
  gradient_norm_threshold: 1.0
  
  # DISABLE PROBLEMATIC FEATURES
  use_label_smoothing: false
  use_huber_loss: false
  
  # CHECKPOINTING
  save_step: 1000
  val_step: 500
  log_step: 25
  
  # EARLY STOPPING
  early_stopping_patience: 20
  early_stopping_min_delta: 0.001

data:
  batch_size: 16  # Smaller batch size for stability
  gradient_accumulation_steps: 4  # Compensate with accumulation
  num_workers: 4
  mixed_precision: false
  
  # Data processing
  sample_rate: 22050
  n_mels: 80
  normalize_audio: true
  trim_silence: true
